# Ensemble Learning: Stacking vs Bagging vs Boosting

This repository contains a full tutorial comparing three ensemble methods—Random Forest (Bagging), XGBoost (Boosting), and Stacking—using the Telco Customer Churn dataset.

## 📁 Files Included
- `Updated_Ensemble_Telco_Churn.ipynb`: Complete Jupyter Notebook with ROC, Confusion Matrices, and F1 comparison
- `requirements.txt`: List of Python packages used in this project

## 🚀 How to Run
1. Clone the repository.
2. Install dependencies using `pip install -r requirements.txt`.
3. Open the notebook and run all cells.

## 📊 Dataset
Telco Customer Churn dataset (available on Kaggle): https://www.kaggle.com/datasets/blastchar/telco-customer-churn

## 🔒 Accessibility
- Colorblind-friendly plots
- Descriptive markdown and captions
- Screen-reader-friendly formatting

## 📚 References
- Breiman (1996): https://doi.org/10.1023/A:1018054314350
- Freund & Schapire (1997): https://doi.org/10.1006/jcss.1997.1504
- Wolpert (1992): https://doi.org/10.1016/S0893-6080(05)80023-1
- Zhou (2012): https://doi.org/10.1007/978-3-642-38652-7
